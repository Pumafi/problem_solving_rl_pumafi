{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CW0dfyb4k2Xg"
      },
      "source": [
        "## ARC-AGI\n",
        "\n",
        "Ferdinand Bhavsar\n",
        "\n",
        "PhD student, Mines Paris"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVi2EfE3k2Xk"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1JVLK7tNqDU"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import math\n",
        "from tqdm.notebook import trange, tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors\n",
        "\n",
        "\n",
        "from scipy.stats import kde\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.utils import to_categorical\n",
        "from tensorflow.keras import layers, losses\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.models import Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "EeVRPcGGk2Xm"
      },
      "source": [
        "### Utilities\n",
        "\n",
        "Get color map (took from some random code I had lying around, so the colors are not the ones from ARC-AGI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbBq_Ee5N5wS"
      },
      "outputs": [],
      "source": [
        "def get_color_map(number_of_categories=4):\n",
        "    \"\"\"\n",
        "    Get the matplotlib colormap and norm for images visualisation\n",
        "    Args:\n",
        "        number_of_categories: number of facies in the slice\n",
        "\n",
        "    Returns: cmap, norm\n",
        "\n",
        "    \"\"\"\n",
        "    if number_of_categories == 4:\n",
        "        cmap = colors.ListedColormap([\"#FF8000\", \"#CBCB33\", \"#9898E5\", \"#66CB33\"])\n",
        "        bounds = [-0.1, 0.9, 1.9, 2.9, 3.9]\n",
        "    elif number_of_categories == 5:\n",
        "        cmap = colors.ListedColormap([\"#000000\", \"#5387AD\", \"#7DD57E\", \"#F1E33E\", \"#C70000\"])\n",
        "        bounds = [-0.1, 0.9, 1.9, 2.9, 3.9, 4.9]\n",
        "    else:  # 9\n",
        "        cmap = colors.ListedColormap(\n",
        "            [\"#000000\", \"#294255\", \"#5387AD\", \"#6DB6B1\", \"#7DD57E\", \"#B5DF5D\", \"#F1E33E\", \"#F77420\", \"#C70000\"])\n",
        "        bounds = [-0.1, 0.9, 1.9, 2.9, 3.9, 4.9, 5.9, 6.9, 7.9, 8.9]\n",
        "\n",
        "    norm = colors.BoundaryNorm(bounds, cmap.N)\n",
        "\n",
        "    return cmap, norm\n",
        "\n",
        "cmap, norm = get_color_map(number_of_categories=9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HM5y6ILLWGxJ",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Dataset Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "0ubkpW-eWCTp",
        "outputId": "8e8b18ac-1dd8-4a6d-e5d1-3202b1703e77"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './arc-agi_training_challenges.json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-593fac50c6e7>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtraining_challenges\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./arc-agi_training_challenges.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mtraining_solutions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./arc-agi_training_solutions.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mevaluation_challenges\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./arc-agi_evaluation_challenges.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-593fac50c6e7>\u001b[0m in \u001b[0;36mload_json\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './arc-agi_training_challenges.json'"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "training_challenges = load_json('./arc-agi_training_challenges.json')\n",
        "training_solutions = load_json('./arc-agi_training_solutions.json')\n",
        "evaluation_challenges = load_json('./arc-agi_evaluation_challenges.json')\n",
        "\n",
        "print(\"Data loaded successfully.\")\n",
        "print(f\"Training tasks: {len(training_challenges)}\")\n",
        "print(f\"Evaluation tasks: {len(evaluation_challenges)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPUDgGJPAr88"
      },
      "outputs": [],
      "source": [
        "def pad_to_shape(arr, target_shape=(30,30,1)):\n",
        "    \"\"\"\n",
        "    Padding the inputs to a single shape, this will make it easier to manipulate\n",
        "    \"\"\"\n",
        "    paddings = [(0, target_shape[i] - arr.shape[i]) for i in range(len(arr.shape))]\n",
        "\n",
        "    padded_array = tf.pad(\n",
        "        arr, paddings, mode='CONSTANT', constant_values=0\n",
        "    )\n",
        "\n",
        "    return padded_array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmsDrKYHk2Xp"
      },
      "source": [
        "Preprocess the challenge data (I'm not touching the indentation, it was a nightmare of using jupyter AND colab for some tests)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hPfc4jOIlR2"
      },
      "outputs": [],
      "source": [
        "def preprocess_challenge_data(challenge_data, solution_data):\n",
        "  challenge_ids = []\n",
        "\n",
        "  # tuples (test_input, test_output) that are both inputs to solution propositioner\n",
        "  challenge_propositioner_inputs = []\n",
        "\n",
        "  # solver trainining input (might be useful)\n",
        "  train_solver_inputs = []\n",
        "  train_solver_outputs = []\n",
        "\n",
        "  #solver test inputs (what the solver will train, getting also a solution as input)\n",
        "  test_solver_inputs = []\n",
        "  test_solver_outputs = []\n",
        "\n",
        "  for id, challenge in challenge_data.items():\n",
        "    challenge_ids.append(id)\n",
        "\n",
        "    # TRAIN\n",
        "    current_challenge_propositioner_inputs = []\n",
        "    current_train_solver_inputs = []\n",
        "    current_train_solver_outputs = []\n",
        "\n",
        "    for train in challenge['train']:\n",
        "      # input\n",
        "      array = np.array(train['input'])\n",
        "\n",
        "      if array.shape[-1] == 1:\n",
        "        # Necessary or to_categorical will mess up the last dim\n",
        "        array = np.expand_dims(array, axis=-1)\n",
        "      array = pad_to_shape(array)\n",
        "      input_cat_tensor = tf.keras.utils.to_categorical(array, num_classes=10)\n",
        "      current_train_solver_inputs.append(input_cat_tensor)\n",
        "\n",
        "      # output\n",
        "      array = np.array(train['output'])\n",
        "\n",
        "      if array.shape[-1] == 1:\n",
        "        array = np.expand_dims(array, axis=-1)\n",
        "      array = pad_to_shape(array)\n",
        "      output_cat_tensor = tf.keras.utils.to_categorical(array, num_classes=10)\n",
        "      current_train_solver_outputs.append(output_cat_tensor)\n",
        "\n",
        "      current_challenge_propositioner_inputs.append((input_cat_tensor, output_cat_tensor))\n",
        "\n",
        "    challenge_propositioner_inputs.append(current_challenge_propositioner_inputs)\n",
        "    train_solver_inputs.append(current_train_solver_inputs)\n",
        "    train_solver_outputs.append(current_train_solver_outputs)\n",
        "\n",
        "    # test\n",
        "    current_test_solver_inputs = []\n",
        "    current_test_solver_outputs = []\n",
        "    for i, test in enumerate(challenge['test']):\n",
        "      # TEST INPUTS\n",
        "      array = np.array(test['input'])\n",
        "\n",
        "      if array.shape[-1] == 1:\n",
        "        array = np.expand_dims(array, axis=-1)\n",
        "      array = pad_to_shape(array)\n",
        "      input_cat_tensor = tf.keras.utils.to_categorical(array, num_classes=10)\n",
        "      current_test_solver_inputs.append(input_cat_tensor)\n",
        "\n",
        "      # TEST OUTPUTS\n",
        "      array = np.array(solution_data[id][i])\n",
        "\n",
        "      if array.shape[-1] == 1:\n",
        "        array = np.expand_dims(array, axis=-1)\n",
        "      array = pad_to_shape(array)\n",
        "      output_cat_tensor = tf.keras.utils.to_categorical(array, num_classes=10)\n",
        "\n",
        "      current_test_solver_outputs.append(output_cat_tensor)\n",
        "\n",
        "    current_test_solver_inputs = np.array(current_test_solver_inputs)\n",
        "    test_solver_inputs.append(current_test_solver_inputs)\n",
        "\n",
        "    current_test_solver_outputs = np.array(current_test_solver_outputs)\n",
        "    test_solver_outputs.append(current_test_solver_outputs)\n",
        "\n",
        "  return challenge_propositioner_inputs, train_solver_inputs, train_solver_outputs, test_solver_inputs, test_solver_outputs\n",
        "      #break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mkd47bFYeN78"
      },
      "outputs": [],
      "source": [
        "challenge_propositioner_inputs, train_solver_inputs, train_solver_outputs, test_solver_inputs, test_solver_outputs= preprocess_challenge_data(training_challenges, training_solutions)\n",
        "print(len(challenge_propositioner_inputs), len(train_solver_inputs), len(train_solver_outputs), len(test_solver_inputs), len(test_solver_outputs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "d_-iLGpxk2Xr"
      },
      "source": [
        "### Plotting some padded data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGplfomyb3hQ"
      },
      "outputs": [],
      "source": [
        "plt.subplot(1, 2, 1)\n",
        "plt.title('input')\n",
        "plt.imshow(np.argmax(challenge_propositioner_inputs[0][0][0], axis=-1), interpolation='nearest', cmap=cmap, norm=norm)\n",
        "plt.axis('off')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('output')\n",
        "plt.imshow(np.argmax(challenge_propositioner_inputs[0][0][1], axis=-1), interpolation='nearest', cmap=cmap, norm=norm)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('input')\n",
        "plt.imshow(np.argmax(train_solver_inputs[0][1], axis=-1), interpolation='nearest', cmap=cmap, norm=norm)\n",
        "plt.axis('off')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('output')\n",
        "plt.imshow(np.argmax(train_solver_outputs[0][1], axis=-1), interpolation='nearest', cmap=cmap, norm=norm)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('input')\n",
        "plt.imshow(np.argmax(test_solver_inputs[0][0], axis=-1), interpolation='nearest', cmap=cmap, norm=norm)\n",
        "plt.axis('off')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('output')\n",
        "plt.imshow(np.argmax(test_solver_outputs[0][0], axis=-1), interpolation='nearest', cmap=cmap, norm=norm)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKelMfUxAAck"
      },
      "outputs": [],
      "source": [
        "def crop_indices(arr):\n",
        "    if len(arr.shape) > 2:\n",
        "      arr = arr.reshape((arr.shape[0], arr.shape[1]))\n",
        "    non_zero_indices = np.argwhere(arr)\n",
        "\n",
        "    if non_zero_indices.size == 0:\n",
        "        return np.array([[0, 30], [0, 30]])\n",
        "\n",
        "    min_indices = list(non_zero_indices.min(axis=0))\n",
        "\n",
        "\n",
        "    max_indices = list(non_zero_indices.max(axis=0) + 1)\n",
        "\n",
        "    res = np.array([min_indices, max_indices])\n",
        "\n",
        "    return res\n",
        "\n",
        "def encoder_preprocess_challenge_data(data):\n",
        "  encoder_training_data = []\n",
        "\n",
        "  for id, challenge in data.items():\n",
        "\n",
        "    for train in challenge['train']:\n",
        "      # input\n",
        "      array = np.array(train['input'])\n",
        "      if array.shape[-1] == 1:\n",
        "        array = np.expand_dims(array, axis=-1)\n",
        "      array = pad_to_shape(array)\n",
        "      cat_tensor = tf.keras.utils.to_categorical(array, num_classes=10)\n",
        "\n",
        "      encoder_training_data.append(cat_tensor)\n",
        "\n",
        "      # outpu\n",
        "      array = np.array(train['output'])\n",
        "      if array.shape[-1] == 1:\n",
        "        array = np.expand_dims(array, axis=-1)\n",
        "      array = pad_to_shape(array)\n",
        "      cat_tensor = tf.keras.utils.to_categorical(array, num_classes=10)\n",
        "\n",
        "      encoder_training_data.append(cat_tensor)\n",
        "\n",
        "\n",
        "  return np.array(encoder_training_data)\n",
        "      #break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GiCdRbj3AK1N"
      },
      "outputs": [],
      "source": [
        "encoder_training_data = encoder_preprocess_challenge_data(training_challenges)\n",
        "print(len(encoder_training_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iSTxkRnYc8Y",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Data AutoEncoder\n",
        "\n",
        "Doing an autoencoder seems like a good idea, after all it's easier working in a latent space. Of course, it probably will not be perfect at all. Maybe in the future, I can use a ppre-trained encoder ?\n",
        "\n",
        "My design choice is not to do dimensionality reduction, as I feel it would take unecessary energy for something that I want simply encoded in a continuous space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sO34Dy3tMvBC"
      },
      "outputs": [],
      "source": [
        "def compute_block(y, filters, activation, kernel_size=(3, 3), strides=(1, 1)):\n",
        "    \"\"\"\n",
        "    The simple computatio nal bloc for both models\n",
        "    \"\"\"\n",
        "    #y = keras.layers.BatchNormalization()(y)\n",
        "    y = layers.Conv2D(\n",
        "        filters, kernel_size, strides=strides, padding=\"same\", kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4), bias_regularizer=regularizers.L2(1e-4),)(y)\n",
        "    #y = layers.Dropout(0.3)(y)\n",
        "    if activation is not None:\n",
        "      y = activation(y)\n",
        "\n",
        "    return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4UDs00fk2Xs"
      },
      "outputs": [],
      "source": [
        "def get_encoder_model(output_channels, kernel_size=(1, 1), layers_features=None):\n",
        "    if layers_features is None:\n",
        "        layers_features = [32, 64, 128]\n",
        "\n",
        "    input = layers.Input(shape=(None, None, 10), name=\"input\")\n",
        "\n",
        "    y = compute_block(input, layers_features[0], tf.keras.layers.LeakyReLU(alpha=0.2), kernel_size=kernel_size)\n",
        "\n",
        "    for i, nb_features in enumerate(layers_features[1:]):\n",
        "        y = compute_block(y, nb_features, tf.keras.layers.LeakyReLU(alpha=0.2), kernel_size=kernel_size)\n",
        "\n",
        "    # Final Bloc -> img\n",
        "    output = compute_block(y, output_channels, None, kernel_size=(1, 1))\n",
        "\n",
        "    encoder = keras.models.Model(input, output, name=\"encoder\")\n",
        "    return encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZjerZhDYInM"
      },
      "outputs": [],
      "source": [
        "def get_decoder_model(output_channels, kernel_size=(1, 1), layers_features=None,\n",
        "                        final_activation=tf.keras.activations.softmax):\n",
        "    if layers_features is None:\n",
        "        layers_features = [128, 64, 10]\n",
        "\n",
        "    input = layers.Input(shape=(None, None, 128), name=\"input\")\n",
        "\n",
        "    y = compute_block(input, layers_features[0], tf.keras.layers.LeakyReLU(alpha=0.2), kernel_size=kernel_size)\n",
        "\n",
        "    for i, nb_features in enumerate(layers_features[1:]):\n",
        "        y = compute_block(y, nb_features, tf.keras.layers.LeakyReLU(alpha=0.2), kernel_size=kernel_size)\n",
        "\n",
        "    # Final Bloc -> img\n",
        "    output = compute_block(y, output_channels, final_activation, kernel_size=(1, 1))\n",
        "\n",
        "    decoder = keras.models.Model(input, output, name=\"decoder\")\n",
        "    return decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KOEXTp9Obwi"
      },
      "outputs": [],
      "source": [
        "class AutoEncoder(keras.Model):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(AutoEncoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, data):\n",
        "        z = encoder(data)\n",
        "        pred_x = decoder(z)\n",
        "        return pred_x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJlx07NDnObX"
      },
      "outputs": [],
      "source": [
        "ae_latent_size = 128 # Should be enough, I'm not doing dimensionality reduction\n",
        "\n",
        "convolution_nb_per_layers = [128, 16]\n",
        "decoder = get_decoder_model(output_channels=10, layers_features=convolution_nb_per_layers)\n",
        "\n",
        "convolution_nb_per_layers = [16, 128,]\n",
        "encoder = get_encoder_model(output_channels=ae_latent_size, layers_features=convolution_nb_per_layers)\n",
        "\n",
        "autoencoder = AutoEncoder(encoder, decoder)\n",
        "autoencoder.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-4), loss=losses.CategoricalCrossentropy())\n",
        "autoencoder.build([None, None, None, 10])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#encoder_checkpoint_path = \"cp-encoder.weights.h5\"\n",
        "\n",
        "#autoencoder.load_weights(encoder_checkpoint_path)"
      ],
      "metadata": {
        "id": "bbjf4XCfEBFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7oxj-m6YfIZ",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzWPyr78aN73"
      },
      "outputs": [],
      "source": [
        "validation_split = 0.2\n",
        "\n",
        "x_train = encoder_training_data[math.floor(len(encoder_training_data) * (1 - validation_split)):]\n",
        "x_test = encoder_training_data[:math.floor(len(encoder_training_data) * (1 - validation_split))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEmgsJ1pCFOE"
      },
      "outputs": [],
      "source": [
        "# 600 was enough before overfitting. I put regularization to avoid it, but don't have time to design a perfect AE\n",
        "if True:\n",
        "  epochs=600\n",
        "\n",
        "  autoencoder.fit(x_train, x_train,\n",
        "                  epochs=epochs,\n",
        "                  shuffle=True,\n",
        "                  validation_data=(x_test, x_test))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "!mkdir encoder_weights\n",
        "\n",
        "encoder_checkpoint_path = \"encoder_weights/cp-encoder-1k1.weights.h5\"\n",
        "encoder_checkpoint_dir = os.path.dirname(encoder_checkpoint_path)\n",
        "\n",
        "autoencoder.save_weights(encoder_checkpoint_path)\n",
        "\n",
        "!tar -czvf encoder_weights.tar.gz ./encoder_weights\n",
        "\n",
        "autoencoder.save(\"autoencoder.keras\")"
      ],
      "metadata": {
        "id": "xeDh8Td7z9iZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "BZeE8BqSk2Xt"
      },
      "source": [
        "#### Visualize some of AE data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAK0ZT4wNVeC"
      },
      "outputs": [],
      "source": [
        "def crop_padded_zeros(arr1, arr2):\n",
        "    \"\"\"\n",
        "    Cropping two arrays to the same dimension. I do this to outputs, but this is only a temporary solution: some outputs size simply depend\n",
        "    on the input size. I cannot be bothered to think about that now. TODO\n",
        "    \"\"\"\n",
        "    non_zero_indices = np.argwhere(arr1)\n",
        "\n",
        "    if non_zero_indices.size == 0:\n",
        "        return np.array([])\n",
        "\n",
        "    min_indices = non_zero_indices.min(axis=0)\n",
        "    max_indices = non_zero_indices.max(axis=0) + 1\n",
        "\n",
        "    cropped_arr1 = arr1[min_indices[0]:max_indices[0], min_indices[1]:max_indices[1], min_indices[2]:max_indices[2]]\n",
        "    cropped_arr2 = arr2[min_indices[0]:max_indices[0], min_indices[1]:max_indices[1], min_indices[2]:max_indices[2]]\n",
        "\n",
        "    return cropped_arr1, cropped_arr2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30oz3KH5k2Xt"
      },
      "source": [
        "Testing if the autoencoder is satisfactory on its validation data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTWTBfjBFX3R"
      },
      "outputs": [],
      "source": [
        "decoded_imgs = autoencoder(x_test, training=False).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCCHu7-9FhRs"
      },
      "outputs": [],
      "source": [
        "for i in range(200):\n",
        "  cropped_x, cropped_pred_x = crop_padded_zeros(x_test[i], decoded_imgs[i])\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.title('input')\n",
        "  plt.imshow(np.argmax(cropped_x, axis=-1), interpolation='nearest', cmap=cmap, norm=norm)\n",
        "  plt.axis('off')\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.title('output')\n",
        "  plt.imshow(np.argmax(cropped_pred_x, axis=-1), interpolation='nearest', cmap=cmap, norm=norm)\n",
        "  plt.axis('off')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNWsSPElk2Xu"
      },
      "source": [
        "### Solution Auto-Encoder\n",
        "\n",
        "This is the beginning of the solver, but it's actually quite simply another AE. The only difference is that this model:\n",
        "\n",
        "-Has an encoder that receives two inputs: the examples (encoded)\n",
        "\n",
        "-The decoder receives two inputs: the input, and the encoded solution\n",
        "\n",
        "-During training, we train the decoder on the solution predicted by the encoder, but with different examples than the encoder\n",
        "\n",
        "NB: At first I tried to do it with a VAE, hoping that learning a distribution would let me search a probability solution space (a bit like the Inverse Problem) btu I couldn't make it work..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZ1lzsLTk2Xu"
      },
      "source": [
        "#### Solver Encoder and Decoder models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiNnXqawkGaJ"
      },
      "outputs": [],
      "source": [
        "def compute_block(y, filters, activation, kernel_size=(3, 3), strides=(1, 1), bn=False, dropout=True):\n",
        "    \"\"\"\n",
        "    The compute bloc for the solver auto-encoder doesn't really change, except for some parameters.\n",
        "    This was done for tests purposes.\n",
        "    \"\"\"\n",
        "    if bn:\n",
        "      y = keras.layers.BatchNormalization()(y)\n",
        "\n",
        "    y = layers.Conv2D(\n",
        "        filters, kernel_size, strides=strides, padding=\"same\", kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4), bias_regularizer=regularizers.L2(1e-4),)(y)\n",
        "    if dropout:\n",
        "        y = layers.Dropout(0.3)(y)\n",
        "    if activation is not None:\n",
        "      y = activation(y)\n",
        "\n",
        "    return y\n",
        "\n",
        "\n",
        "def get_proposition_model(output_channels, kernel_size=(3, 3), layers_features=None):\n",
        "    if layers_features is None:\n",
        "        layers_features = [512, 256, 256, 128, 128]\n",
        "\n",
        "    # Two inputs, the examples\n",
        "    train_input = layers.Input(shape=(30, 30, 128), name=\"train_input\")\n",
        "    train_output = layers.Input(shape=(30, 30, 128), name=\"train_output\")\n",
        "\n",
        "    y_input = compute_block(train_input, layers_features[0] // 2, tf.keras.layers.LeakyReLU(alpha=0.2), kernel_size=kernel_size)\n",
        "    y_output = compute_block(train_output, layers_features[0] // 2, tf.keras.layers.LeakyReLU(alpha=0.2), kernel_size=kernel_size)\n",
        "    y = tf.keras.layers.Concatenate(axis=-1)([y_input, y_output])\n",
        "\n",
        "    y = compute_block(y, layers_features[0], tf.keras.layers.LeakyReLU(alpha=0.2), kernel_size=kernel_size)\n",
        "\n",
        "    for i, nb_features in enumerate(layers_features[1:]):\n",
        "        y = compute_block(y, nb_features, tf.keras.layers.LeakyReLU(alpha=0.2), kernel_size=kernel_size)\n",
        "\n",
        "    # Final Bloc -> img\n",
        "    output = compute_block(y, output_channels, tf.keras.layers.LeakyReLU(alpha=0.2), kernel_size=(3, 3))\n",
        "\n",
        "\n",
        "    proposition_model = keras.models.Model([train_input, train_output], output, name=\"proposition_model\")\n",
        "    return proposition_model\n",
        "\n",
        "def get_solver_model(input_channels, output_channels, kernel_size=(3, 3), layers_features=None):\n",
        "    # nb_prop * channels_solutions (128) * channels_input (128) 163840\n",
        "    if layers_features is None:\n",
        "        layers_features = [128, 128, 256, 256, 512]\n",
        "\n",
        "    encoded_solution = layers.Input(shape=(30, 30, input_channels), name=\"encoded_solution\")\n",
        "    encoded_input = layers.Input(shape=(30, 30, output_channels), name=\"encoded_input\")\n",
        "\n",
        "    y_solut = compute_block(encoded_solution, layers_features[0] // 2, tf.keras.layers.LeakyReLU(alpha=0.2), kernel_size=kernel_size)\n",
        "    y_input = compute_block(encoded_input, layers_features[0] // 2, tf.keras.layers.LeakyReLU(alpha=0.2), kernel_size=kernel_size)\n",
        "    y = tf.keras.layers.Concatenate(axis=-1)([y_solut, y_input])\n",
        "\n",
        "    y = compute_block(y, layers_features[0], tf.keras.layers.LeakyReLU(alpha=0.2), kernel_size=kernel_size)\n",
        "\n",
        "    for i, nb_features in enumerate(layers_features[1:]):\n",
        "        y = compute_block(y, nb_features, tf.keras.layers.LeakyReLU(alpha=0.2), kernel_size=kernel_size)\n",
        "\n",
        "    # Final Bloc -> img\n",
        "    output = compute_block(y, output_channels, tf.keras.layers.LeakyReLU(alpha=0.2), kernel_size=(3, 3), dropout=False)\n",
        "\n",
        "    solver_model = keras.models.Model([encoded_solution, encoded_input], output, name=\"solver_model\")\n",
        "    return solver_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "keAhtiDck2Xu"
      },
      "outputs": [],
      "source": [
        "# Kernel initializer to use\n",
        "def kernel_init(scale):\n",
        "    scale = max(scale, 1e-10)\n",
        "    return keras.initializers.VarianceScaling(\n",
        "        scale, mode=\"fan_avg\", distribution=\"uniform\"\n",
        "    )\n",
        "\n",
        "\n",
        "class AttentionBlock(layers.Layer):\n",
        "    \"\"\"Applies self-attention.\n",
        "\n",
        "    Args:\n",
        "        units: Number of units in the dense layers\n",
        "        groups: Number of groups to be used for GroupNormalization layer\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, units, groups=8, name=\"AttentionBlock\", **kwargs):\n",
        "        self.units = units\n",
        "        self.groups = groups\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.norm = layers.GroupNormalization(groups=groups, name=name+\"-Norm\")\n",
        "        self.query = layers.Dense(units, kernel_initializer=kernel_init(1.0), name=name+\"-Query\")\n",
        "        self.key = layers.Dense(units, kernel_initializer=kernel_init(1.0), name=name+\"-Key\")\n",
        "        self.value = layers.Dense(units, kernel_initializer=kernel_init(1.0), name=name+\"-Value\")\n",
        "        self.proj = layers.Dense(units, kernel_initializer=kernel_init(0.0), name=name+\"-Proj\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        height = tf.shape(inputs)[1]\n",
        "        width = tf.shape(inputs)[2]\n",
        "        scale = tf.cast(self.units, tf.float32) ** (-0.5)\n",
        "\n",
        "        inputs = self.norm(inputs)\n",
        "        q = self.query(inputs)\n",
        "        k = self.key(inputs)\n",
        "        v = self.value(inputs)\n",
        "\n",
        "        attn_score = tf.einsum(\"bhwc, bHWc->bhwHW\", q, k) * scale\n",
        "        attn_score = tf.reshape(attn_score, [batch_size, height, width, height * width])\n",
        "\n",
        "        attn_score = tf.nn.softmax(attn_score, -1)\n",
        "        attn_score = tf.reshape(attn_score, [batch_size, height, width, height, width])\n",
        "\n",
        "        proj = tf.einsum(\"bhwHW,bHWc->bhwc\", attn_score, v)\n",
        "        proj = self.proj(proj)\n",
        "        return inputs + proj\n",
        "\n",
        "\n",
        "def ResidualBlock(width, groups=8, activation_fn=keras.activations.swish, dropout=True, name=\"ResBlock\"):\n",
        "    def apply(inputs):\n",
        "        x, sol = inputs\n",
        "        input_width = x.shape[3]\n",
        "\n",
        "        if input_width == width:\n",
        "            residual = x\n",
        "        else:\n",
        "            residual = layers.Conv2D(\n",
        "                width, kernel_size=1, kernel_initializer=kernel_init(1.0), name=name+\"-Conv0\"\n",
        "            )(x)\n",
        "\n",
        "        sol = activation_fn(sol)\n",
        "        sol = layers.Conv2D(\n",
        "            width,\n",
        "            kernel_size=(3, 3),\n",
        "            padding=\"same\",\n",
        "        )(sol)\n",
        "\n",
        "        x = activation_fn(x)\n",
        "        x = layers.Conv2D(\n",
        "            width, kernel_size=3, padding=\"same\", kernel_initializer=kernel_init(1.0), name=name+\"-Conv1\"\n",
        "        )(x)\n",
        "        if dropout:\n",
        "            x = layers.Dropout(0.3, name=name+\"-Dropout1\")(x)\n",
        "\n",
        "        x = layers.Concatenate(axis=-1)([x, sol])\n",
        "        x = activation_fn(x)\n",
        "        x = layers.Conv2D(\n",
        "            width, kernel_size=3, padding=\"same\", kernel_initializer=kernel_init(0.0), name=name+\"-ConvConcat\"\n",
        "        )(x)\n",
        "        #x = layers.Add(name=name+\"-Add1\")([x, sol])\n",
        "        x = activation_fn(x)\n",
        "\n",
        "        x = layers.Conv2D(\n",
        "            width, kernel_size=3, padding=\"same\", kernel_initializer=kernel_init(0.0), name=name+\"-Conv2\"\n",
        "        )(x)\n",
        "        x = layers.Add(name=name+\"-Add2\")([x, residual])\n",
        "        return x\n",
        "\n",
        "    return apply\n",
        "\n",
        "\n",
        "def DownSample(width, name=\"DownSample\"):\n",
        "    def apply(x):\n",
        "        x = layers.Conv2D(\n",
        "            width,\n",
        "            kernel_size=3,\n",
        "            strides=2,\n",
        "            padding=\"same\",\n",
        "            kernel_initializer=kernel_init(1.0), name=name+\"-Conv\"\n",
        "        )(x)\n",
        "        return x\n",
        "\n",
        "    return apply\n",
        "\n",
        "\n",
        "def UpSample(width, interpolation=\"nearest\", name=\"UpSample\"):\n",
        "    def apply(x):\n",
        "        x = layers.UpSampling2D(size=2, interpolation=interpolation, name=name+\"-UpSamp\")(x)\n",
        "        x = layers.Conv2D(\n",
        "            width, kernel_size=3, padding=\"same\", kernel_initializer=kernel_init(1.0), name=name+\"-Conv\"\n",
        "        )(x)\n",
        "        return x\n",
        "\n",
        "    return apply\n",
        "\n",
        "def get_solver_model(input_channels, output_channels,\n",
        "                     widths, has_attention, upsampling,\n",
        "                     kernel_size=(3, 3),\n",
        "                     num_res_blocks=2,\n",
        "                     norm_groups=8,\n",
        "                     nb_channels=9,\n",
        "                     interpolation=\"nearest\",\n",
        "                     activation_fn=keras.activations.swish,):\n",
        "\n",
        "    if widths is None:\n",
        "        widths = [128, 128, 256, 256]\n",
        "\n",
        "    encoded_solution = layers.Input(shape=(30, 30, input_channels), name=\"encoded_solution\")\n",
        "    encoded_input = layers.Input(shape=(30, 30, output_channels), name=\"encoded_input\")\n",
        "\n",
        "    x = layers.Conv2D(\n",
        "        widths[0],\n",
        "        kernel_size=(3, 3),\n",
        "        padding=\"same\",\n",
        "        kernel_initializer=kernel_init(1.0),\n",
        "        name=\"InitialConv2D_X\"\n",
        "    )(encoded_solution)\n",
        "\n",
        "    sol = layers.Conv2D(\n",
        "        widths[0],\n",
        "        kernel_size=(3, 3),\n",
        "        padding=\"same\",\n",
        "        kernel_initializer=kernel_init(1.0),\n",
        "        name=\"InitialConv2D_SOL\"\n",
        "    )(encoded_input)\n",
        "\n",
        "    skips = [x]\n",
        "    sol_save = sol\n",
        "\n",
        "    # DownBlock\n",
        "    print(\"DOWN\")\n",
        "    for i in range(len(widths)):\n",
        "        print(i)\n",
        "        for j in range(num_res_blocks):\n",
        "            x = ResidualBlock(\n",
        "                widths[i], groups=norm_groups, activation_fn=activation_fn, name=\"ResBlock-Down-{0}-{1}\".format(i, j)\n",
        "            )([x, sol])\n",
        "            if has_attention[i]:\n",
        "                x = AttentionBlock(widths[i], groups=norm_groups, name=\"AttentionBlock-Down-{0}-{1}\".format(i, j))(x)\n",
        "            skips.append(x)\n",
        "\n",
        "        if upsampling[i]:\n",
        "            x = DownSample(widths[i], name=\"DownSampleX-{0}\".format(i))(x)\n",
        "            sol = layers.Conv2D(widths[i], kernel_size=(3, 3), padding=\"same\", strides=2)(sol_save) #DownSample(widths[i], name=\"DownSampleSol-{0}\".format(i))(sol_save)\n",
        "            skips.append(x)\n",
        "    # MiddleBlock\n",
        "    x = ResidualBlock(widths[-1], groups=norm_groups, activation_fn=activation_fn, dropout=False, name=\"MidResBlock1\")(\n",
        "        [x, sol]\n",
        "    )\n",
        "    x = AttentionBlock(widths[-1], groups=norm_groups, name=\"MidAttBlock\")(x)\n",
        "    x = ResidualBlock(widths[-1], groups=norm_groups, activation_fn=activation_fn, dropout=False, name=\"MidResBlock2\")(\n",
        "        [x, sol]\n",
        "    )\n",
        "\n",
        "    # UpBlock\n",
        "    for i in reversed(range(len(widths))):\n",
        "        print(i)\n",
        "        for j in range(num_res_blocks):\n",
        "\n",
        "            skip = skips.pop()\n",
        "            x = layers.Concatenate(axis=-1, name=\"Up-Concat-{0}-{1}\".format(i, j))([x, skip])\n",
        "            x = ResidualBlock(\n",
        "                widths[i], groups=norm_groups, activation_fn=activation_fn, name=\"ResBlock-Up-{0}-{1}\".format(i, j)\n",
        "            )([x, sol])\n",
        "            if has_attention[i]:\n",
        "                x = AttentionBlock(widths[i], groups=norm_groups, name=\"AttentionBlock-Up-{0}-{1}\".format(i, j))(x)\n",
        "\n",
        "            if upsampling[i] and j == 0:\n",
        "                x = UpSample(widths[i], interpolation=interpolation, name=\"UpSampleX-{0}\".format(i))(x)\n",
        "                sol =  layers.Conv2DTranspose(widths[i], kernel_size=3,\n",
        "                                              strides=2, activation=tf.keras.layers.LeakyReLU(alpha=0.2), padding='same')(sol)\n",
        "                #sol_save #UpSample(widths[i], interpolation=interpolation, name=\"UpSampleSol-{0}\".format(i))(sol_save)\n",
        "\n",
        "    # End block\n",
        "    x = layers.Conv2D(output_channels, (3, 3), padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.2))(x)\n",
        "    x = layers.Conv2D(output_channels, (3, 3), padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.2), name=\"FinalConv2D\")(x)\n",
        "    #x = tf.cast(x, tf.float64)\n",
        "    return keras.Model([encoded_solution, encoded_input], x, name=\"unet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "ml7b7Gh2k2Xv"
      },
      "source": [
        "#### Data for solver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yaenaygc5_IA"
      },
      "outputs": [],
      "source": [
        "def solver_encoder_preprocess_challenge_data(data):\n",
        "  solver_encoder_training_data_input = []\n",
        "  solver_encoder_training_data_output = []\n",
        "  solver_encoder_training_data_ids = []\n",
        "\n",
        "  for id, challenge in data.items():\n",
        "\n",
        "    for train in challenge['train']:\n",
        "      # input\n",
        "      solver_encoder_training_data_ids.append(id)\n",
        "      array = np.array(train['input'])\n",
        "      if array.shape[-1] == 1:\n",
        "        array = np.expand_dims(array, axis=-1)\n",
        "      array = pad_to_shape(array)\n",
        "      cat_tensor = tf.keras.utils.to_categorical(array, num_classes=10)\n",
        "\n",
        "      solver_encoder_training_data_input.append(cat_tensor)\n",
        "\n",
        "      # outpu\n",
        "      array = np.array(train['output'])\n",
        "      if array.shape[-1] == 1:\n",
        "        array = np.expand_dims(array, axis=-1)\n",
        "      array = pad_to_shape(array)\n",
        "      cat_tensor = tf.keras.utils.to_categorical(array, num_classes=10)\n",
        "\n",
        "      solver_encoder_training_data_output.append(cat_tensor)\n",
        "  encoded_solver_encoder_training_data_input = autoencoder.encoder(np.array(solver_encoder_training_data_input))\n",
        "  encoded_solver_encoder_training_data_output = autoencoder.encoder(np.array(solver_encoder_training_data_output))\n",
        "  return encoded_solver_encoder_training_data_input, encoded_solver_encoder_training_data_output, tf.squeeze(np.array(solver_encoder_training_data_ids))\n",
        "      #break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCu4PREE6ga6"
      },
      "outputs": [],
      "source": [
        "solver_encoder_training_data_input, solver_encoder_training_data_output, solver_encoder_training_data_ids = solver_encoder_preprocess_challenge_data(training_challenges)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_x2yPnL7ZZp"
      },
      "outputs": [],
      "source": [
        "class DataGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self, solver_encoder_training_data_ids,\n",
        "                 solver_encoder_training_data_input,\n",
        "                 solver_encoder_training_data_output,\n",
        "                 batch_size=32,\n",
        "                 input_dim=(30, 30, 256),\n",
        "                 output_dim=(30, 30, 128),\n",
        "                 shuffle=True):\n",
        "        self.solver_encoder_training_data_ids = solver_encoder_training_data_ids.numpy()\n",
        "        self.solver_encoder_training_data_input = solver_encoder_training_data_input.numpy()\n",
        "        self.solver_encoder_training_data_output = solver_encoder_training_data_output.numpy()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def get_random_challenge_output(self, ids):\n",
        "        random_indices = []\n",
        "\n",
        "        # Iterate over each id in the ids_list\n",
        "        for id_str in ids:\n",
        "            # Find all indices where the id_str is located in the ids array\n",
        "            indices = tf.squeeze(tf.where(tf.equal(self.solver_encoder_training_data_ids, id_str)))\n",
        "\n",
        "            # Randomly select one index if there are multiple\n",
        "            if tf.size(indices) > 0:\n",
        "                random_index = tf.random.shuffle(indices)\n",
        "                random_indices.append(random_index[0])\n",
        "\n",
        "        return tf.gather(self.solver_encoder_training_data_input, random_indices), tf.gather(self.solver_encoder_training_data_output, random_indices)\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(len(self.solver_encoder_training_data_input) / self.batch_size))\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "\n",
        "        if self.shuffle:\n",
        "            p = np.random.permutation(len(self.solver_encoder_training_data_ids))\n",
        "            self.solver_encoder_training_data_ids = tf.gather(solver_encoder_training_data_ids, p)\n",
        "            self.solver_encoder_training_data_input = tf.gather(solver_encoder_training_data_input, p)\n",
        "            self.solver_encoder_training_data_output = tf.gather(solver_encoder_training_data_output, p)\n",
        "        #if self.shuffle:\n",
        "        #    np.random.shuffle(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        ids = self.solver_encoder_training_data_ids[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "        input_input = self.solver_encoder_training_data_input[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "        input_output = self.solver_encoder_training_data_output[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "\n",
        "        random_output_rate = np.random.rand()\n",
        "        if random_output_rate > 0.4:\n",
        "            output_input, output_output = self.get_random_challenge_output(ids)\n",
        "        else:\n",
        "            output_input = self.solver_encoder_training_data_input[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "            output_output = self.solver_encoder_training_data_output[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "\n",
        "        return (input_input, input_output), (output_input, output_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMqoUkKFk2Xw"
      },
      "outputs": [],
      "source": [
        "training_datagenerator = DataGenerator(solver_encoder_training_data_ids[:int(np.floor(len(solver_encoder_training_data_ids) * 0.80))],\n",
        "                                       solver_encoder_training_data_input[:int(np.floor(len(solver_encoder_training_data_ids) * 0.80))],\n",
        "                                       solver_encoder_training_data_output[:int(np.floor(len(solver_encoder_training_data_ids) * 0.80))])\n",
        "\n",
        "validation_generator = DataGenerator(solver_encoder_training_data_ids[int(np.ceil(len(solver_encoder_training_data_ids) * 0.80)):],\n",
        "                                       solver_encoder_training_data_input[int(np.ceil(len(solver_encoder_training_data_ids) * 0.80)):],\n",
        "                                       solver_encoder_training_data_output[int(np.ceil(len(solver_encoder_training_data_ids) * 0.80)):])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDAhrq04k2Xw"
      },
      "source": [
        "#### Solver AE model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FQzZIkOk2Xw"
      },
      "outputs": [],
      "source": [
        "class SolverAutoEncoder(keras.Model):\n",
        "    def __init__(self, proposition_model, solver_model):\n",
        "        super(SolverAutoEncoder, self).__init__()\n",
        "        self.proposition_model = proposition_model\n",
        "        self.solver_model = solver_model\n",
        "        self.teacher_prop = 0.7\n",
        "\n",
        "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
        "            name=\"reconstruction_loss\"\n",
        "        )\n",
        "\n",
        "        #self.solver_encoder_training_data_ids = tf.squeeze(solver_encoder_training_data_ids)\n",
        "        #self.solver_encoder_training_data_output = tf.squeeze(solver_encoder_training_data_output)\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, x):\n",
        "        encoded_train_input, encoded_train_output, _ = x\n",
        "        solution = self.proposition_model((encoded_train_input, encoded_train_output), training=False)\n",
        "\n",
        "        # Apply solution\n",
        "        solved_encoded_train_output = self.solver_model((solution, encoded_train_input), training=False)\n",
        "        return solved_encoded_train_output\n",
        "\n",
        "    @tf.function\n",
        "    def get_random_challenge_output(self, ids):\n",
        "        random_indices = []\n",
        "\n",
        "        # Iterate over each id in the ids_list\n",
        "        for id_str in ids:\n",
        "            # Find all indices where the id_str is located in the ids array\n",
        "            indices = tf.squeeze(tf.where(tf.equal(self.solver_encoder_training_data_ids, id_str)))\n",
        "\n",
        "            # Randomly select one index if there are multiple\n",
        "            if tf.size(indices) > 0:\n",
        "                random_index = tf.random.shuffle(indices)\n",
        "                random_indices.append(random_index[0])\n",
        "\n",
        "        return tf.gather(self.solver_encoder_training_data_output, random_indices)\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.reconstruction_loss_tracker,\n",
        "        ]\n",
        "\n",
        "    def train_step(self, data):\n",
        "        inputs, outputs = data\n",
        "        encoded_train_input, encoded_train_output = inputs\n",
        "        encoded_test_input, encoded_test_output = outputs\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            solution = self.proposition_model((encoded_train_input, encoded_train_output))\n",
        "\n",
        "            solved_encoded_train_output = self.solver_model((solution, encoded_test_input))\n",
        "\n",
        "            reconstruction_loss = keras.losses.MeanSquaredError()(solved_encoded_train_output, encoded_test_output)\n",
        "\n",
        "        grads = tape.gradient(reconstruction_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        return {\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "        }\n",
        "\n",
        "    def test_step(self, data):\n",
        "        inputs, outputs = data\n",
        "        encoded_train_input, encoded_train_output = inputs\n",
        "        encoded_test_input, encoded_test_output = outputs\n",
        "\n",
        "\n",
        "        solution = self.proposition_model((encoded_train_input, encoded_train_output), training=False)\n",
        "\n",
        "        solved_encoded_train_output = self.solver_model((solution, encoded_test_input), training=False)\n",
        "\n",
        "\n",
        "        reconstruction_loss = keras.losses.MeanSquaredError()(solved_encoded_train_output, encoded_test_output)\n",
        "\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        return {\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NcLseAfk2Xx"
      },
      "outputs": [],
      "source": [
        "solution_latent_size = 256\n",
        "solutioner = get_proposition_model(solution_latent_size, kernel_size=(3, 3))\n",
        "solver = get_solver_model(solution_latent_size, ae_latent_size,\n",
        "                          widths = [256, 256, 128, 128,],\n",
        "                          has_attention= [False, False, True, True,],\n",
        "                          upsampling=[True, False, False, False],\n",
        "                          kernel_size=(3, 3))\n",
        "solver_autoencoder = SolverAutoEncoder(solutioner, solver)\n",
        "\n",
        "solver_autoencoder.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-5), loss=losses.MeanSquaredError())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDwqgBYHE43s"
      },
      "outputs": [],
      "source": [
        "test = solver_autoencoder((tf.expand_dims(solver_encoder_training_data_input[0], axis=0),\n",
        "                           tf.expand_dims(solver_encoder_training_data_output[0], axis=0),\n",
        "                           tf.expand_dims(solver_encoder_training_data_output[0], axis=0)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#solver_autoencoder_checkpoint_path = \"cp-solver.weights.h5\"\n",
        "\n",
        "#solver_autoencoder.load_weights(solver_autoencoder_checkpoint_path)"
      ],
      "metadata": {
        "id": "zeDyDw9sEKBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "solver_autoencoder.optimizer.learning_rate = 1e-6"
      ],
      "metadata": {
        "id": "ll5_u31aSGCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbbr3b9wBq7w"
      },
      "outputs": [],
      "source": [
        "if True:\n",
        "  epochs=1000\n",
        "\n",
        "  solver_autoencoder.fit(training_datagenerator,\n",
        "                        epochs=epochs,\n",
        "                        batch_size=32,\n",
        "                        shuffle=True,\n",
        "                        validation_data=validation_generator\n",
        "                        )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "0.1618\n",
        "0.1601\n",
        "0.1578"
      ],
      "metadata": {
        "id": "--tfT9o5g2rG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "!mkdir solver_weights\n",
        "\n",
        "solver_checkpoint_path = \"solver_weights/cp-solver.weights.h5\"\n",
        "solver_checkpoint_dir = os.path.dirname(solver_checkpoint_path)\n",
        "\n",
        "solver_autoencoder.save_weights(solver_checkpoint_path)\n",
        "\n",
        "!tar -czvf solver_weights.tar.gz ./solver_weights\n",
        "\n",
        "solver_autoencoder.save(\"autosolver.keras\")"
      ],
      "metadata": {
        "id": "O-y5_nANEIka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfzkhogPk2Xz"
      },
      "outputs": [],
      "source": [
        "for i in range(5):\n",
        "  output_test_solver = tf.expand_dims(challenge_propositioner_inputs[i][1][1], axis=0)\n",
        "  input_test_solver = tf.expand_dims(challenge_propositioner_inputs[i][1][0], axis=0)\n",
        "\n",
        "  encoded_input_test_solver = autoencoder.encoder(input_test_solver, training=False).numpy()\n",
        "  encoded_output_test_solver = autoencoder.encoder(output_test_solver, training=False).numpy()\n",
        "\n",
        "  solved_encoded_output = solver_autoencoder((encoded_input_test_solver, encoded_output_test_solver, None))\n",
        "  solved_output = autoencoder.decoder(solved_encoded_output, training=False).numpy()\n",
        "\n",
        "  cropped_input, _ = crop_padded_zeros(input_test_solver[0], input_test_solver[0])\n",
        "  cropped_output, cropped_pred_output = crop_padded_zeros(output_test_solver[0], solved_output[0])\n",
        "  plt.subplot(1, 3, 1)\n",
        "  plt.title('input')\n",
        "  plt.imshow(np.argmax(cropped_input, axis=-1), interpolation='nearest', cmap=cmap, norm=norm)\n",
        "  plt.axis('off')\n",
        "  plt.subplot(1, 3, 2)\n",
        "  plt.title('output')\n",
        "  plt.imshow(np.argmax(cropped_output, axis=-1), interpolation='nearest', cmap=cmap, norm=norm)\n",
        "  plt.axis('off')\n",
        "  plt.subplot(1, 3, 3)\n",
        "  plt.title('output')\n",
        "  plt.imshow(np.argmax(cropped_pred_output, axis=-1), interpolation='nearest', cmap=cmap, norm=norm)\n",
        "  plt.axis('off')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r6Ls86nM_pSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "hl6mTLfdk2Xz"
      },
      "source": [
        "### OTHER TESTS"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_to_shape(arr, target_shape=(30,30,1)):\n",
        "    \"\"\"\n",
        "    Padding the inputs to a single shape, this will make it easier to manipulate\n",
        "    \"\"\"\n",
        "    paddings = [(0, target_shape[i] - arr.shape[i]) for i in range(len(arr.shape))]\n",
        "\n",
        "    padded_array = tf.pad(\n",
        "        arr, paddings, mode='CONSTANT', constant_values=0\n",
        "    )\n",
        "\n",
        "    return padded_array"
      ],
      "metadata": {
        "id": "MjtZmfbr_xU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOAqhRcY_5En"
      },
      "outputs": [],
      "source": [
        "def solver_encoder_preprocess_challenge_data(data):\n",
        "  solver_encoder_training_data_input = []\n",
        "  solver_encoder_training_data_output = []\n",
        "  solver_encoder_training_data_ids = []\n",
        "\n",
        "  for id, challenge in data.items():\n",
        "\n",
        "    for train in challenge['train']:\n",
        "      # input\n",
        "      solver_encoder_training_data_ids.append(id)\n",
        "      array = np.array(train['input'])\n",
        "      #if array.shape[-1] == 1:\n",
        "      #  array = np.expand_dims(array, axis=-1)\n",
        "      array = pad_to_shape(array)\n",
        "      cat_tensor = tf.keras.utils.to_categorical(array, num_classes=10)\n",
        "\n",
        "      solver_encoder_training_data_input.append(cat_tensor)\n",
        "\n",
        "      # outpu\n",
        "      array = np.array(train['output'])\n",
        "      #if array.shape[-1] == 1:\n",
        "      #  array = np.expand_dims(array, axis=-1)\n",
        "      array = pad_to_shape(array)\n",
        "      cat_tensor = tf.keras.utils.to_categorical(array, num_classes=10)\n",
        "\n",
        "      solver_encoder_training_data_output.append(cat_tensor)\n",
        "  encoded_solver_encoder_training_data_input = np.array(solver_encoder_training_data_input)\n",
        "  encoded_solver_encoder_training_data_output = np.array(solver_encoder_training_data_output)\n",
        "  return encoded_solver_encoder_training_data_input, encoded_solver_encoder_training_data_output, tf.squeeze(np.array(solver_encoder_training_data_ids))\n",
        "      #break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C29FzLqe_5Eu"
      },
      "outputs": [],
      "source": [
        "solver_encoder_training_data_input, solver_encoder_training_data_output, solver_encoder_training_data_ids = solver_encoder_preprocess_challenge_data(training_challenges)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "solver_encoder_training_data_output.shape"
      ],
      "metadata": {
        "id": "7YQ364_uEk0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title('input')\n",
        "plt.imshow(np.argmax(solver_encoder_training_data_output[100], axis=-1), interpolation='nearest', cmap=cmap, norm=norm)\n",
        "plt.axis('off')"
      ],
      "metadata": {
        "id": "Hme72SUyPVuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtQ-d1zm_5Ev"
      },
      "outputs": [],
      "source": [
        "class DataGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self, solver_encoder_training_data_ids,\n",
        "                 solver_encoder_training_data_input,\n",
        "                 solver_encoder_training_data_output,\n",
        "                 batch_size=32,\n",
        "                 input_dim=(30, 30, 256),\n",
        "                 output_dim=(30, 30, 128),\n",
        "                 shuffle=True):\n",
        "        self.solver_encoder_training_data_ids = solver_encoder_training_data_ids\n",
        "        self.solver_encoder_training_data_input = solver_encoder_training_data_input\n",
        "        self.solver_encoder_training_data_output = solver_encoder_training_data_output\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def get_random_challenge_output(self, ids):\n",
        "        random_indices = []\n",
        "\n",
        "        # Iterate over each id in the ids_list\n",
        "        for id_str in ids:\n",
        "            # Find all indices where the id_str is located in the ids array\n",
        "            indices = tf.squeeze(tf.where(tf.equal(self.solver_encoder_training_data_ids, id_str)))\n",
        "\n",
        "            # Randomly select one index if there are multiple\n",
        "            if tf.size(indices) > 0:\n",
        "                random_index = tf.random.shuffle(indices)\n",
        "                random_indices.append(random_index[0])\n",
        "\n",
        "        return tf.gather(self.solver_encoder_training_data_input, random_indices), tf.gather(self.solver_encoder_training_data_output, random_indices)\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(len(self.solver_encoder_training_data_input) / self.batch_size))\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "\n",
        "        if self.shuffle:\n",
        "            p = np.random.permutation(len(self.solver_encoder_training_data_ids))\n",
        "            self.solver_encoder_training_data_ids = tf.gather(solver_encoder_training_data_ids, p)\n",
        "            self.solver_encoder_training_data_input = tf.gather(solver_encoder_training_data_input, p)\n",
        "            self.solver_encoder_training_data_output = tf.gather(solver_encoder_training_data_output, p)\n",
        "        #if self.shuffle:\n",
        "        #    np.random.shuffle(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        ids = self.solver_encoder_training_data_ids[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "        input_input = self.solver_encoder_training_data_input[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "        input_output = self.solver_encoder_training_data_output[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "\n",
        "        random_output_rate = np.random.rand()\n",
        "        if random_output_rate > 0.4:\n",
        "            output_input, output_output = self.get_random_challenge_output(ids)\n",
        "        else:\n",
        "            output_input = self.solver_encoder_training_data_input[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "            output_output = self.solver_encoder_training_data_output[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "\n",
        "        return (input_input, input_output), (output_input, output_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9doAW2HF_5Ev"
      },
      "outputs": [],
      "source": [
        "training_datagenerator = DataGenerator(solver_encoder_training_data_ids[:int(np.floor(len(solver_encoder_training_data_ids) * 0.80))],\n",
        "                                       solver_encoder_training_data_input[:int(np.floor(len(solver_encoder_training_data_ids) * 0.80))],\n",
        "                                       solver_encoder_training_data_output[:int(np.floor(len(solver_encoder_training_data_ids) * 0.80))])\n",
        "\n",
        "validation_generator = DataGenerator(solver_encoder_training_data_ids[int(np.ceil(len(solver_encoder_training_data_ids) * 0.80)):],\n",
        "                                       solver_encoder_training_data_input[int(np.ceil(len(solver_encoder_training_data_ids) * 0.80)):],\n",
        "                                       solver_encoder_training_data_output[int(np.ceil(len(solver_encoder_training_data_ids) * 0.80)):])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SolverFinalAutoEncoder(keras.Model):\n",
        "    def __init__(self, proposition_model, solver_model, encoder, decoder):\n",
        "        super(SolverFinalAutoEncoder, self).__init__()\n",
        "        self.proposition_model = proposition_model\n",
        "        self.solver_model = solver_model\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.teacher_prop = 0.7\n",
        "\n",
        "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
        "            name=\"reconstruction_loss\",\n",
        "        )\n",
        "\n",
        "        self.encoding_loss_tracker = keras.metrics.Mean(\n",
        "            name=\"encoding_loss\",\n",
        "        )\n",
        "\n",
        "        #self.solver_encoder_training_data_ids = tf.squeeze(solver_encoder_training_data_ids)\n",
        "        #self.solver_encoder_training_data_output = tf.squeeze(solver_encoder_training_data_output)\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, x):\n",
        "        train_input, train_output, _ = x\n",
        "        encoded_train_input = self.encoder(train_input)\n",
        "        encoded_train_output = self.encoder(train_output)\n",
        "        solution = self.proposition_model((encoded_train_input, encoded_train_output), training=False)\n",
        "\n",
        "        # Apply solution\n",
        "        solved_encoded_train_output = self.solver_model((solution, encoded_train_input), training=False)\n",
        "        return solved_encoded_train_output\n",
        "\n",
        "    @tf.function\n",
        "    def get_random_challenge_output(self, ids):\n",
        "        random_indices = []\n",
        "\n",
        "        # Iterate over each id in the ids_list\n",
        "        for id_str in ids:\n",
        "            # Find all indices where the id_str is located in the ids array\n",
        "            indices = tf.squeeze(tf.where(tf.equal(self.solver_encoder_training_data_ids, id_str)))\n",
        "\n",
        "            # Randomly select one index if there are multiple\n",
        "            if tf.size(indices) > 0:\n",
        "                random_index = tf.random.shuffle(indices)\n",
        "                random_indices.append(random_index[0])\n",
        "\n",
        "        return tf.gather(self.solver_encoder_training_data_output, random_indices)\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.reconstruction_loss_tracker,\n",
        "            self.encoding_loss_tracker,\n",
        "        ]\n",
        "\n",
        "    def train_step(self, data):\n",
        "        inputs, outputs = data\n",
        "        train_input, train_output = inputs\n",
        "        test_input, test_output = outputs\n",
        "        with tf.GradientTape() as tape:\n",
        "            encoded_train_input = self.encoder(train_input)\n",
        "            encoded_train_output = self.encoder(train_output)\n",
        "            encoded_test_input = self.encoder(test_input)\n",
        "            encoded_test_output = self.encoder(test_output)\n",
        "            solution = self.proposition_model((encoded_train_input, encoded_train_output))\n",
        "\n",
        "            solved_encoded_train_output = self.solver_model((solution, encoded_test_input))\n",
        "\n",
        "            reconstruction_loss = keras.losses.MeanSquaredError()(solved_encoded_train_output, encoded_test_output)\n",
        "\n",
        "            decoded_train_input = self.decoder(encoded_train_input)\n",
        "            decoded_train_output = self.decoder(encoded_train_output)\n",
        "\n",
        "            encoding_loss = (keras.losses.CategoricalCrossentropy()(decoded_train_input, train_input) + keras.losses.CategoricalCrossentropy()(decoded_train_output, train_output))/2\n",
        "            loss = 0.1*encoding_loss + reconstruction_loss\n",
        "        grads = tape.gradient(loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "\n",
        "        # with tf.GradientTape() as tape:\n",
        "        #    solution = self.proposition_model((encoded_train_input, encoded_train_output))\n",
        "\n",
        "        #    solved_encoded_train_output = self.solver_model((solution, encoded_test_input))\n",
        "\n",
        "        #    reconstruction_loss = keras.losses.MeanSquaredError()(solved_encoded_train_output, encoded_test_output)\n",
        "\n",
        "        #grads = tape.gradient(reconstruction_loss, self.trainable_weights)\n",
        "        #self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.encoding_loss_tracker.update_state(encoding_loss)\n",
        "        return {\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "            \"encoding_loss\": self.encoding_loss_tracker.result(),\n",
        "        }\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.reconstruction_loss_tracker,\n",
        "            self.encoding_loss_tracker,\n",
        "        ]\n",
        "\n",
        "    def test_step(self, data):\n",
        "        inputs, outputs = data\n",
        "        train_input, train_output = inputs\n",
        "        test_input, test_output = outputs\n",
        "        encoded_train_input = self.encoder(train_input)\n",
        "        encoded_train_output = self.encoder(train_output)\n",
        "        encoded_test_input = self.encoder(test_input)\n",
        "        encoded_test_output = self.encoder(test_output)\n",
        "        solution = self.proposition_model((encoded_train_input, encoded_train_output))\n",
        "\n",
        "        solved_encoded_train_output = self.solver_model((solution, encoded_test_input))\n",
        "\n",
        "        reconstruction_loss = keras.losses.MeanSquaredError()(solved_encoded_train_output, encoded_test_output)\n",
        "\n",
        "        decoded_train_input = self.decoder(encoded_train_input)\n",
        "        decoded_train_output = self.decoder(encoded_train_output)\n",
        "\n",
        "        encoding_loss = (keras.losses.CategoricalCrossentropy()(decoded_train_input, train_input) + keras.losses.CategoricalCrossentropy()(decoded_train_output, train_output))/2\n",
        "        loss = 0.1*encoding_loss + reconstruction_loss\n",
        "\n",
        "        # with tf.GradientTape() as tape:\n",
        "        #    solution = self.proposition_model((encoded_train_input, encoded_train_output))\n",
        "\n",
        "        #    solved_encoded_train_output = self.solver_model((solution, encoded_test_input))\n",
        "\n",
        "        #    reconstruction_loss = keras.losses.MeanSquaredError()(solved_encoded_train_output, encoded_test_output)\n",
        "\n",
        "        #grads = tape.gradient(reconstruction_loss, self.trainable_weights)\n",
        "        #self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.encoding_loss_tracker.update_state(encoding_loss)\n",
        "        return {\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "            \"encoding_loss\": self.encoding_loss_tracker.result(),\n",
        "        }"
      ],
      "metadata": {
        "id": "dwPMCY2k_xZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "solver_final_autoencoder = SolverFinalAutoEncoder(solutioner, solver, autoencoder.encoder, autoencoder.decoder)\n",
        "\n",
        "solver_final_autoencoder.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-5), loss=losses.MeanSquaredError())"
      ],
      "metadata": {
        "id": "5qsY8vx8_xfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rn77zQE9_xiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkIozvrPDFLj"
      },
      "outputs": [],
      "source": [
        "test = solver_final_autoencoder((tf.expand_dims(solver_encoder_training_data_input[0], axis=0),\n",
        "                           tf.expand_dims(solver_encoder_training_data_output[0], axis=0),\n",
        "                           tf.expand_dims(solver_encoder_training_data_output[0], axis=0)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decode_solved = solver_final_autoencoder.decoder(test)"
      ],
      "metadata": {
        "id": "z-FKn9VooXyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title('input')\n",
        "plt.imshow(np.argmax(solver_encoder_training_data_output[0], axis=-1), interpolation='nearest', cmap=cmap, norm=norm)\n",
        "plt.axis('off')"
      ],
      "metadata": {
        "id": "ZiHXo3jLoul0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title('input')\n",
        "plt.imshow(np.argmax(decode_solved[0], axis=-1), interpolation='nearest', cmap=cmap, norm=norm)\n",
        "plt.axis('off')"
      ],
      "metadata": {
        "id": "zDM5ISfiolG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPMTUegJDFLs"
      },
      "outputs": [],
      "source": [
        "epochs=1000\n",
        "\n",
        "solver_final_autoencoder.fit(training_datagenerator,\n",
        "                       epochs=epochs,\n",
        "                       batch_size=32,\n",
        "                       shuffle=True,\n",
        "                       validation_data=validation_generator\n",
        "                       )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KTSLmPmGa2kS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P52NmIbJbOhx"
      },
      "outputs": [],
      "source": [
        "class SolverAutoEncoder(keras.Model):\n",
        "    def __init__(self, proposition_model, solver_model):\n",
        "        super(SolverAutoEncoder, self).__init__()\n",
        "        self.proposition_model = proposition_model\n",
        "        self.solver_model = solver_model\n",
        "        #self.sampling_layer = Sampling()\n",
        "        self.teacher_prop = 0.7\n",
        "        #self.solver_encoder_training_data_ids = tf.squeeze(solver_encoder_training_data_ids)\n",
        "        #self.solver_encoder_training_data_output = tf.squeeze(solver_encoder_training_data_output)\n",
        "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
        "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
        "            name=\"reconstruction_loss\"\n",
        "        )\n",
        "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, x):\n",
        "        ids, encoded_train_input, encoded_train_output = x\n",
        "        z_mean, z_log_var = self.proposition_model((encoded_train_input, encoded_train_output))\n",
        "        solution = self.sampling_layer((z_mean, z_log_var))\n",
        "\n",
        "        # Apply solution\n",
        "        solved_encoded_train_output = self.solver_model((solution, encoded_train_input))\n",
        "        return solved_encoded_train_output\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.total_loss_tracker,\n",
        "            self.reconstruction_loss_tracker,\n",
        "            #self.kl_loss_tracker,\n",
        "        ]\n",
        "\n",
        "    def train_step(self, data):\n",
        "        print(\"debug\")\n",
        "        if isinstance(data, tuple):\n",
        "            ids, encoded_train_input, encoded_train_output = data[0]\n",
        "        else:\n",
        "            ids, encoded_train_input, encoded_train_output = data\n",
        "        print(\"debug0\")\n",
        "        #if random.random() > self.teacher_prop:\n",
        "        #    goal_outputs = self.get_random_challenge_output(ids)\n",
        "        #else:\n",
        "        #  goal_outputs = encoded_train_output\n",
        "        goal_outputs = encoded_train_output\n",
        "        print(\"debug1\")\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            solution = self.proposition_model((encoded_train_input, encoded_train_output))\n",
        "\n",
        "            #solution = self.sampling_layer((z_mean, z_log_var))\n",
        "            solved_encoded_train_output = self.solver_model((solution, encoded_train_input))\n",
        "\n",
        "\n",
        "            reconstruction_loss = keras.losses.MeanSquaredError()(solved_encoded_train_output, goal_outputs)\n",
        "            #kl_loss = -0.5 * (1 + z_log_var - tf.keras.backend.square(z_mean) - tf.keras.backend.exp(z_log_var))\n",
        "\n",
        "            #kl_loss = tf.math.reduce_mean(tf.math.reduce_sum(kl_loss, axis=1))\n",
        "\n",
        "            total_loss = reconstruction_loss #+ kl_loss\n",
        "        print(\"debug2\")\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        #self.kl_loss_tracker.update_state(kl_loss)\n",
        "        return {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "        }\n",
        "\n",
        "    def test_step(self, data):\n",
        "        print(\"debug\")\n",
        "        if isinstance(data, tuple):\n",
        "            ids, encoded_train_input, encoded_train_output = data[0]\n",
        "        else:\n",
        "            ids, encoded_train_input, encoded_train_output = data\n",
        "        print(\"debug0\")\n",
        "\n",
        "        #if random.random() > self.teacher_prop:\n",
        "        #    goal_outputs = self.get_random_challenge_output(ids)\n",
        "        #else:\n",
        "        #  goal_outputs = encoded_train_output\n",
        "        goal_outputs = encoded_train_output\n",
        "\n",
        "        solution = self.proposition_model((encoded_train_input, encoded_train_output), training=False)\n",
        "\n",
        "        #solution = self.sampling_layer((z_mean, z_log_var))\n",
        "        solved_encoded_train_output = self.solver_model((solution, encoded_train_input), training=False)\n",
        "\n",
        "\n",
        "        reconstruction_loss = keras.losses.MeanSquaredError()(solved_encoded_train_output, goal_outputs)\n",
        "        #kl_loss = -0.5 * (1 + z_log_var - tf.keras.backend.square(z_mean) - tf.keras.backend.exp(z_log_var))\n",
        "\n",
        "        #kl_loss = tf.math.reduce_mean(tf.math.reduce_sum(kl_loss, axis=1))\n",
        "\n",
        "        total_loss = reconstruction_loss #+ kl_loss\n",
        "\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.solver_tracker.update_state(reconstruction_loss)\n",
        "        #self.kl_loss_tracker.update_state(kl_loss)\n",
        "        return {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLBZbWOEPrZR"
      },
      "outputs": [],
      "source": [
        "def get_updater_model(input_channels, output_channels, nb_proposition_backtracking, kernel_size=(3, 3), layers_features=None):\n",
        "    if layers_features is None:\n",
        "        layers_features = [64, 128, 128, 128]\n",
        "\n",
        "    distr_mean_input = layers.Input(shape=(30, 30, output_channels), name=\"distr_mean_input\")\n",
        "    distr_std_input = layers.Input(shape=(30, 30, output_channels), name=\"distr_std_input\")\n",
        "    y_mean = compute_block(distr_mean_input, layers_features[0], tf.keras.layers.LeakyReLU(alpha=0.2), kernel_size=kernel_size)\n",
        "    y_std = compute_block(distr_std_input, layers_features[0], tf.keras.layers.LeakyReLU(alpha=0.2), kernel_size=kernel_size)\n",
        "    y_dist = tf.keras.layers.Concatenate(axis=-1)([y_mean, y_std])\n",
        "    y_dist = compute_block(y_dist, layers_features[0], tf.keras.layers.LeakyReLU(alpha=0.2), kernel_size=kernel_size)\n",
        "\n",
        "    y_sol = None\n",
        "    train_inputs = []\n",
        "    for i in range(nb_proposition_backtracking):\n",
        "        train_input = layers.Input(shape=(30, 30, input_channels), name=\"solution_input_{}\".format(i))\n",
        "        train_inputs.append(train_input)\n",
        "        y_tmp = compute_block(train_input, layers_features[0], tf.keras.layers.LeakyReLU(alpha=0.2), kernel_size=kernel_size)\n",
        "\n",
        "        if y_sol is None:\n",
        "            y_sol = y_tmp\n",
        "        else:\n",
        "            y_sol = tf.keras.layers.Concatenate(axis=-1)([y_sol, y_tmp])\n",
        "    y_sol = compute_block(y_sol, layers_features[0], tf.keras.layers.LeakyReLU(alpha=0.2), kernel_size=kernel_size)\n",
        "\n",
        "    y = tf.keras.layers.Concatenate(axis=-1)([y_dist, y_sol])\n",
        "    yy = compute_block(y, layers_features[0], tf.keras.layers.LeakyReLU(alpha=0.2), kernel_size=kernel_size)\n",
        "\n",
        "    #train_input = layers.Input(shape=(30, 30, 128), name=\"train_input\")\n",
        "    #train_output = layers.Input(shape=(30, 30, 128), name=\"train_output\")\n",
        "\n",
        "    #y_input = compute_block(train_input, layers_features[0], tf.keras.layers.LeakyReLU(alpha=0.2), kernel_size=kernel_size)\n",
        "    #y_output = compute_block(train_output, layers_features[0], tf.keras.layers.LeakyReLU(alpha=0.2), kernel_size=kernel_size)\n",
        "    #y = tf.keras.layers.Concatenate(axis=-1)([y_input, y_output])\n",
        "\n",
        "    for i, nb_features in enumerate(layers_features[1:]):\n",
        "        y = compute_block(y, nb_features, tf.keras.layers.LeakyReLU(alpha=0.2), kernel_size=kernel_size)\n",
        "\n",
        "    # Final Bloc -> img\n",
        "    mean = compute_block(y, output_channels, None, kernel_size=(3, 3))\n",
        "    z_log_var  = compute_block(y, output_channels, None, kernel_size=(3, 3))\n",
        "\n",
        "    proposition_model = keras.models.Model([distr_mean_input, distr_std_input, *train_inputs], [mean, z_log_var], name=\"proposition_model\")\n",
        "    return proposition_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnPOQ-BPkM9E"
      },
      "outputs": [],
      "source": [
        "class PropositionalSolver(keras.Model):\n",
        "    def __init__(self, proposition_model, solver_model, autoencoder, solution_updater_model):\n",
        "        super(PropositionalSolver, self).__init__()\n",
        "\n",
        "        self.solution_proposer_model = proposition_model\n",
        "        self.solver_model = solver_model\n",
        "\n",
        "        self.solution_updater_model = solution_updater_model\n",
        "        self.autoencoder = autoencoder\n",
        "        self.nb_proposition_backtracking = 5\n",
        "        self.nb_research_iterations = 1\n",
        "        self.memory_size = 20\n",
        "\n",
        "        self.seed_generator = keras.random.SeedGenerator(1337)\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, inputs):\n",
        "        train_challenge_input, train_challenge_outputs, test_challenge_inputs = inputs\n",
        "\n",
        "        if train_challenge_input.shape[0] != train_challenge_outputs.shape[0]:\n",
        "            raise Exception(\"Training data input and output have different size\")\n",
        "\n",
        "        # ENCODING\n",
        "        encoded_train_input = autoencoder.encoder(train_challenge_input)\n",
        "        encoded_train_output = autoencoder.encoder(train_challenge_outputs)\n",
        "\n",
        "        # SOLUTION DISTRIBUTION\n",
        "        z_mean, z_log_var = self.solution_proposer_model((encoded_train_input, encoded_train_output))\n",
        "\n",
        "        # Keep track of previous best solutions\n",
        "        best_sol = np.zeros((self.memory_size, 30, 30, 128))\n",
        "        best_losses_reducted = np.full((self.memory_size), 5**9)\n",
        "        best_losses = np.zeros((self.memory_size, 30, 30, 128)) * 5**9\n",
        "        for i in tqdm(range(self.nb_research_iterations)):\n",
        "            #Sample solution\n",
        "            epsilon = keras.random.normal(shape=z_mean.shape, seed=self.seed_generator)\n",
        "            solution = z_mean + keras.ops.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "            # Apply solution\n",
        "            solved_encoded_train_output = self.solver_model((solution, encoded_train_input))\n",
        "            # Compute the loss on the solution\n",
        "            solving_loss = tf.square(encoded_train_output - solved_encoded_train_output)\n",
        "            reduced_loss =  tf.reduce_sum(solving_loss, axis=[1, 2, 3])\n",
        "\n",
        "            # Concat the current solutions to best solutions to make a filter that keeps only the best\n",
        "            best_sol = tf.concat([best_sol, solution], axis=0)\n",
        "            best_losses_reducted = tf.concat([best_losses_reducted, reduced_loss], axis=0)\n",
        "            best_losses = tf.concat([best_losses, solving_loss], axis=0)\n",
        "            # Sort the solutions by the sum of the MSE\n",
        "            sorted_indices = tf.argsort(best_losses_reducted, axis=0)\n",
        "            best_sol = tf.gather(best_sol, sorted_indices)\n",
        "            best_losses_reducted = tf.gather(best_losses_reducted, sorted_indices)\n",
        "            best_losses = tf.gather(best_losses, sorted_indices)\n",
        "            # Keep only the best 20 solutions\n",
        "            best_sol = best_sol[:self.memory_size]\n",
        "            best_losses_reducted = best_losses_reducted[:self.memory_size]\n",
        "            best_losses = best_losses[:self.memory_size]\n",
        "\n",
        "            # Select 5 random solutions and their loss from the 20 best solutions to serve as input to the update model\n",
        "            selected_solutions_input = None\n",
        "            for j in range(z_mean.shape[0]): # Iterate on batch dimension. Didn't have time to think of a better solution\n",
        "              random_solutions_indices = tf.random.shuffle(tf.range(self.memory_size))[:self.nb_proposition_backtracking]\n",
        "              selected_solutions = tf.gather(best_sol, random_solutions_indices)\n",
        "              selected_losses = tf.gather(best_losses, random_solutions_indices)\n",
        "              concatenated_sol_loss = tf.expand_dims(tf.concat([selected_solutions, selected_losses], axis=-1), axis=1)\n",
        "              if selected_solutions_input is None:\n",
        "                  selected_solutions_input = concatenated_sol_loss\n",
        "              else:\n",
        "                  selected_solutions_input = tf.concat([selected_solutions_input, concatenated_sol_loss], axis=1)\n",
        "            # Final selected solutions\n",
        "            selected_solutions_input = tuple(tf.unstack(selected_solutions_input))\n",
        "\n",
        "            # Update distribution based on previous distribution but also on memory\n",
        "            z_mean, z_log_var = update_model((z_mean, z_log_var, *selected_solutions_input))\n",
        "\n",
        "            #kl_loss = -0.5 * (1 + z_log_var - tf.ops.square(z_mean) - tf.ops.exp(z_log_var))\n",
        "            #kl_loss = tf.ops.mean(ops.sum(kl_loss, axis=1))\n",
        "            #total_loss = reconstruction_loss\n",
        "        encoded_test_input = autoencoder.encoder(test_challenge_inputs)\n",
        "        # Apply best solution\n",
        "        chosen_solution = tf.repeat(tf.expand_dims(best_sol[0], axis=0), encoded_test_input.shape[0], axis=0)\n",
        "        solved_encoded_test_output = self.solver_model((chosen_solution, encoded_test_input))\n",
        "        solved_output = autoencoder.decoder(solved_encoded_test_output)\n",
        "\n",
        "\n",
        "        return solved_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMJ-ObdMkWLu"
      },
      "outputs": [],
      "source": [
        "\n",
        "update_model = get_updater_model(solution_latent_size * 2, solution_latent_size, nb_proposition_backtracking=5)\n",
        "\n",
        "prop_solver = PropositionalSolver(solutioner, solver, autoencoder, update_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QK1RpuDomQMc"
      },
      "outputs": [],
      "source": [
        "prop_solver((np.array(train_solver_inputs[0]),\n",
        "             np.array(train_solver_outputs[0]),\n",
        "             np.array(test_solver_inputs[0])\n",
        "             ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UeJeIQLLkqmA"
      },
      "outputs": [],
      "source": [
        "len(train_solver_inputs), train_solver_inputs[0][0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXpyfIZ6kNCg"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "optimizer = keras.optimizers.Adam(learning_rate=5e-3)\n",
        "\n",
        "# TODO\n",
        "\n",
        "def fit_encoder(inputs, outputs, ids, batch_size, epochs):\n",
        "    training_data = list(zip(inputs, outputs, ids))\n",
        "    print(len(training_data))\n",
        "    nb_iterations = math.ceil(len(inputs) / batch_size)\n",
        "    indices = list(range(len(inputs)))\n",
        "    for epoch in range(epochs):\n",
        "        random.shuffle(training_data)\n",
        "\n",
        "        batch_indices = np.array_split(indices, nb_iterations)\n",
        "        for iter in range(nb_iterations):\n",
        "          i, j = batch_indices[iter][0], batch_indices[iter][-1] + 1\n",
        "\n",
        "          x, y, curr_id = zip(*(training_data[i:j]))\n",
        "\n",
        "          with tf.GradientTape() as tape:\n",
        "            loss = 0\n",
        "            cce = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
        "            for curr_x in x:\n",
        "              curr_x = tf.expand_dims(curr_x, axis=0)\n",
        "              #curr_y = tf.expand_dims(y[i], axis=0)\n",
        "\n",
        "              pred_x = autoencoder(curr_x)\n",
        "\n",
        "              loss += cce(curr_x, pred_x)\n",
        "          gradient = tape.gradient(loss, autoencoder.trainable_variables)\n",
        "          optimizer.apply_gradients(\n",
        "              zip(gradient, autoencoder.trainable_variables)\n",
        "          )\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}